# ğŸ­ Emote Gesture Detection

Train a CNN to detect your gestures and map them to emojis in real-time!

## ğŸš€ Quick Start

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Collect training data (50+ images per gesture)
python collect_data.py

# 3. Split data into train/val
python split_data.py

# 4. Train your model
python train.py

# 5. Run real-time detection (desktop)
python realtime.py

# 6. Export ONNX for the browser demo (after training)
python export_onnx.py
```

## ğŸ“‹ What You Need

**Before training, collect images of yourself doing each gesture:**
- Target: 50-100 images per gesture
- Move around for variety (angles, lighting, distance)
- The more diverse, the better the model

**Your gestures:**
- 67_emote
- goblin_crying
- wizard_dabbing
- biting_nails
- cheering
- neutral (idle/relaxed)

## ğŸ® Controls

**collect_data.py:**
- `SPACE` - Start/pause capture
- `N` - Next gesture
- `Q` - Quit

**realtime.py:**
- `Q` - Quit

## ğŸ§  How It Works

```
Camera â†’ Face Detection (CNN) â†’ Gesture Recognition (ResNet18 CNN) â†’ Emoji Display
```

1. OpenCV detects faces in webcam
2. ResNet18 CNN recognizes your gesture
3. Matching emoji is displayed

## ğŸ“ Project Structure

```
imagedetection/
â”œâ”€â”€ collect_data.py      # Capture training images
â”œâ”€â”€ split_data.py        # Split train/val (80/20)
â”œâ”€â”€ train.py             # Train ResNet18 CNN
â”œâ”€â”€ export_onnx.py       # Convert PyTorch weights â†’ ONNX + web config
â”œâ”€â”€ realtime.py          # Live detection (OpenCV)
â”œâ”€â”€ labels.json          # Gesture classes
â”œâ”€â”€ emote_map.json       # Gestureâ†’emoji mapping
â”œâ”€â”€ index.html           # Web demo
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train/          # Training images
â”‚   â””â”€â”€ val/            # Validation images
â”œâ”€â”€ emotes/             # Your emoji images
â””â”€â”€ models/             # Trained CNN models
```

## ğŸŒ Browser Demo & Deployment

`index.html` now runs your gesture model directly in the browser via [ONNX Runtime Web](https://onnxruntime.ai/docs/execution-providers/Web.html)â€”no Python backend required.

1. Train the model locally (`python train.py`) and make sure `models/expr_resnet18.pt` exists.
2. Export the weights + config for the web client:
   ```bash
   python export_onnx.py \
     --ckpt models/expr_resnet18.pt \
     --onnx models/expr_resnet18.onnx \
     --config models/web_model_config.json
   ```
3. Host the following files on any static host (e.g., GitHub Pages, Netlify, Vercel):
   - `index.html`
   - `emote_map.json`
   - `emotes/` (emoji images)
   - `models/expr_resnet18.onnx`
   - `models/web_model_config.json`
4. Open the hosted page, click **Start Camera**, and the UI will stream webcam frames, run ONNX inference client-side, show probabilities, and render the detected emote.

Tip: re-run `export_onnx.py` whenever you retrain so the web build stays in sync.
