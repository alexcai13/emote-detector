# ğŸ­ Emote Gesture Detection

Train a CNN to detect your gestures and map them to emojis in real-time!

## ğŸš€ Quick Start

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Collect training data (50+ images per gesture)
python collect_data.py

# 3. Split data into train/val
python split_data.py

# 4. Train your model
python train.py

# 5. Run real-time detection
python realtime.py
```

## ğŸ“‹ What You Need

**Before training, collect images of yourself doing each gesture:**
- Target: 50-100 images per gesture
- Move around for variety (angles, lighting, distance)
- The more diverse, the better the model

**Your gestures:**
- 67_emote
- goblin_crying
- wizard_dabbing
- biting_nails
- cheering
- neutral (idle/relaxed)

## ğŸ® Controls

**collect_data.py:**
- `SPACE` - Start/pause capture
- `N` - Next gesture
- `Q` - Quit

**realtime.py:**
- `Q` - Quit

## ğŸ§  How It Works

```
Camera â†’ Face Detection (CNN) â†’ Gesture Recognition (ResNet18 CNN) â†’ Emoji Display
```

1. OpenCV detects faces in webcam
2. ResNet18 CNN recognizes your gesture
3. Matching emoji is displayed

## ğŸ“ Project Structure

```
imagedetection/
â”œâ”€â”€ collect_data.py      # Capture training images
â”œâ”€â”€ split_data.py        # Split train/val (80/20)
â”œâ”€â”€ train.py             # Train ResNet18 CNN
â”œâ”€â”€ realtime.py          # Live detection
â”œâ”€â”€ labels.json          # Gesture classes
â”œâ”€â”€ emote_map.json       # Gestureâ†’emoji mapping
â”œâ”€â”€ index.html           # Web demo
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train/          # Training images
â”‚   â””â”€â”€ val/            # Validation images
â”œâ”€â”€ emotes/             # Your emoji images
â””â”€â”€ models/             # Trained CNN models
```

## ğŸŒ Web Demo

Open `index.html` for a demo interface (static simulation).

For full live detection, run `realtime.py` locally.
