# ğŸ­ Emote Gesture Detection

Train a CNN to detect your gestures and map them to emojis in real-time!

## ğŸš€ Quick Start

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Collect training data (50+ images per gesture)
python collect_data.py

# 3. Split data into train/val
python split_data.py

# 4. Train your model
python train.py

# 5. Run real-time detection (desktop)
python realtime.py

# 6. (Maintainers only) Export ONNX when you retrain
python export_onnx.py
```

## ğŸ“‹ What You Need

**Before training, collect images of yourself doing each gesture:**
- Target: 50-100 images per gesture
- Move around for variety (angles, lighting, distance)
- The more diverse, the better the model

**Your gestures:**
- 67_emote
- goblin_crying
- wizard_dabbing
- biting_nails
- cheering
- neutral (idle/relaxed)

## ğŸ® Controls

**collect_data.py:**
- `SPACE` - Start/pause capture
- `N` - Next gesture
- `Q` - Quit

**realtime.py:**
- `Q` - Quit

## ğŸ§  How It Works

```
Camera â†’ Face Detection (CNN) â†’ Gesture Recognition (ResNet18 CNN) â†’ Emoji Display
```

1. OpenCV detects faces in webcam
2. ResNet18 CNN recognizes your gesture
3. Matching emoji is displayed

## ğŸ“ Project Structure

```
imagedetection/
â”œâ”€â”€ collect_data.py      # Capture training images
â”œâ”€â”€ split_data.py        # Split train/val (80/20)
â”œâ”€â”€ train.py             # Train ResNet18 CNN
â”œâ”€â”€ export_onnx.py       # Convert PyTorch weights â†’ ONNX + web config
â”œâ”€â”€ realtime.py          # Live detection (OpenCV)
â”œâ”€â”€ labels.json          # Gesture classes
â”œâ”€â”€ emote_map.json       # Gestureâ†’emoji mapping
â”œâ”€â”€ index.html           # Web demo
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train/          # Training images
â”‚   â””â”€â”€ val/            # Validation images
â”œâ”€â”€ emotes/             # Your emoji images
â””â”€â”€ models/             # Trained CNN models
```

## ğŸŒ Browser Demo & Deployment

### Hosted demo (recommended for viewers)
- Just open the deployed GitHub Pages/Netlify URL and click **Start Camera**. Everythingâ€”model download, webcam capture, inferenceâ€”runs directly in your browser. No setup required.

### Updating the hosted model (maintainers only)
1. Train the model locally (`python train.py`) so `models/expr_resnet18.pt` is current.
2. Run:
   ```bash
   python export_onnx.py \
     --ckpt models/expr_resnet18.pt \
     --onnx models/expr_resnet18.onnx \
     --config models/web_model_config.json
   ```
3. Deploy/commit the updated `models/expr_resnet18.onnx` and `models/web_model_config.json` along with `index.html`, `emote_map.json`, and `emotes/`.
4. Reload the hosted pageâ€”viewers automatically get the new model.
