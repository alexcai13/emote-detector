# ğŸ­ Emote Gesture Detection

Train a CNN to detect your gestures and map them to emojis in real-time!

## ğŸš€ Quick Start

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Collect training data (50+ images per gesture)
python collect_data.py

# 3. Split data into train/val
python split_data.py

# 4. Train your model
python train.py

# 5. Run real-time detection (desktop)
python realtime.py

# 6. (Maintainers only) Export ONNX when you retrain
python export_onnx.py
```

## ğŸ“‹ What You Need

**Before training, collect images of yourself doing each gesture:**
- Target: 50-100 images per gesture
- Move around for variety (angles, lighting, distance)
- The more diverse, the better the model

**Your gestures:**
- 67_emote
- goblin_crying
- wizard_dabbing
- biting_nails
- cheering
- neutral 

## ğŸ® Controls

**collect_data.py:**
- `SPACE` - Start/pause capture
- `N` - Next gesture
- `Q` - Quit

**realtime.py:**
- `Q` - Quit

## ğŸ“ Project Structure

```
imagedetection/
â”œâ”€â”€ collect_data.py      # Capture training images
â”œâ”€â”€ split_data.py        # Split train/val (80/20)
â”œâ”€â”€ train.py             # Train ResNet18 CNN
â”œâ”€â”€ export_onnx.py       # Convert PyTorch weights â†’ ONNX + web config
â”œâ”€â”€ realtime.py          # Live detection (OpenCV)
â”œâ”€â”€ labels.json          # Gesture classes
â”œâ”€â”€ emote_map.json       # Gestureâ†’emoji mapping
â”œâ”€â”€ index.html           # Web demo
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train/          # Training images
â”‚   â””â”€â”€ val/            # Validation images
â”œâ”€â”€ emotes/             # Your emoji images
â””â”€â”€ models/             # Trained CNN models
```
